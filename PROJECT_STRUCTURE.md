# WG-Scraper - Projektstruktur

## ğŸ“ Ãœbersicht

```
WG Scraper/
â”‚
â”œâ”€â”€ ğŸ“ Dokumentation
â”‚   â”œâ”€â”€ README_USAGE.md          # VollstÃ¤ndige Bedienungsanleitung
â”‚   â”œâ”€â”€ QUICKSTART.md            # Schnellstart-Guide
â”‚   â”œâ”€â”€ README.rst               # PyScaffold Standard-README
â”‚   â”œâ”€â”€ AUTHORS.rst              # Autoren
â”‚   â”œâ”€â”€ CHANGELOG.rst            # Ã„nderungsprotokoll
â”‚   â”œâ”€â”€ CONTRIBUTING.rst         # BeitrÃ¤ge-Richtlinien
â”‚   â””â”€â”€ LICENSE.txt              # MIT-Lizenz
â”‚
â”œâ”€â”€ ğŸ’» Quellcode (src/wg_scraper/)
â”‚   â”œâ”€â”€ __init__.py              # Paket-Initialisierung & Versionierung
â”‚   â”œâ”€â”€ cli.py                   # âœ… CLI-Befehle (Click)
â”‚   â”œâ”€â”€ scraper.py               # â³ Web-Scraping-Logik (TODO)
â”‚   â”œâ”€â”€ database.py              # âœ… SQLite-Datenbank-Verwaltung
â”‚   â”œâ”€â”€ models.py                # âœ… Datenmodelle (WGListing)
â”‚   â”œâ”€â”€ config.py                # âœ… Konfiguration & Konstanten
â”‚   â””â”€â”€ skeleton.py              # PyScaffold-Beispiel (kann entfernt werden)
â”‚
â”œâ”€â”€ ğŸ§ª Tests
â”‚   â”œâ”€â”€ conftest.py              # Pytest-Konfiguration
â”‚   â””â”€â”€ test_skeleton.py         # Beispiel-Tests
â”‚
â”œâ”€â”€ ğŸ“š Beispiele & Hilfsmittel
â”‚   â”œâ”€â”€ examples.py              # âœ… Beispiel-Script fÃ¼r API-Nutzung
â”‚   â””â”€â”€ requirements-dev.txt     # âœ… Development-Dependencies
â”‚
â”œâ”€â”€ âš™ï¸ Konfiguration
â”‚   â”œâ”€â”€ setup.cfg                # âœ… Package-Konfiguration (aktualisiert)
â”‚   â”œâ”€â”€ setup.py                 # Setup-Script
â”‚   â”œâ”€â”€ pyproject.toml           # Build-System-Konfiguration
â”‚   â”œâ”€â”€ tox.ini                  # Tox-Test-Umgebungen
â”‚   â””â”€â”€ .gitignore               # âœ… Git-Ignore (mit DB-Dateien)
â”‚
â”œâ”€â”€ ğŸ“¦ Installation
â”‚   â””â”€â”€ .env/                    # Virtual Environment
â”‚
â””â”€â”€ ğŸ—„ï¸ Daten (wird erstellt)
    â”œâ”€â”€ wg_data.db               # Produktions-Datenbank
    â””â”€â”€ example_data.db          # Test-Datenbank

Legende:
âœ… = Implementiert und getestet
â³ = Grundstruktur vorhanden, muss angepasst werden
```

## ğŸ¯ Kernmodule

### 1. CLI (cli.py) âœ…

**FunktionalitÃ¤t:**
- `scrape`: WG-Anzeigen von URL scrapen
- `list`: Gespeicherte Anzeigen anzeigen
- `stats`: Statistiken Ã¼ber Anzeigen

**Status:** VollstÃ¤ndig implementiert

**Nutzung:**
```bash
.env/bin/wg-scraper scrape "URL"
.env/bin/wg-scraper list --city Berlin
.env/bin/wg-scraper stats
```

---

### 2. Scraper (scraper.py) â³

**FunktionalitÃ¤t:**
- `WGScraper`: Hauptklasse fÃ¼r Web-Scraping
- `scrape_search_results()`: Iteriert durch Suchergebnisse
- `scrape_listing_details()`: Detail-Seiten scrapen (optional)

**Status:** Grundstruktur mit Platzhaltern

**TODOs:**
- [ ] HTML-Selektoren anpassen
- [ ] Listing-Parsing implementieren
- [ ] Pagination implementieren
- [ ] ID-Extraktion anpassen

**Wichtige Methoden:**
```python
scraper = WGScraper(delay=1.0)
for listing in scraper.scrape_search_results(url, max_pages=5):
    print(listing)
```

---

### 3. Database (database.py) âœ…

**FunktionalitÃ¤t:**
- SQLite-Datenbank-Verwaltung
- CRUD-Operationen fÃ¼r Listings
- Statistiken und Filterung

**Status:** VollstÃ¤ndig implementiert

**API:**
```python
db = Database("wg_data.db")
db.init_db()
db.save_listing(listing)
listings = db.get_listings(city="Berlin", max_rent=500)
stats = db.get_statistics()
```

---

### 4. Models (models.py) âœ…

**FunktionalitÃ¤t:**
- `WGListing`: Dataclass fÃ¼r WG-Anzeigen
- Serialisierung (to_dict/from_dict)

**Status:** VollstÃ¤ndig implementiert

**Felder:**
- `listing_id`, `url`, `title` (Pflicht)
- `city`, `size`, `rent`, `available_from` (Optional)
- Viele weitere optionale Felder

---

### 5. Config (config.py) âœ…

**FunktionalitÃ¤t:**
- Zentrale Konfiguration
- CSS-Selektoren (mÃ¼ssen angepasst werden!)
- Umgebungsvariablen-Support

**Status:** Grundstruktur vorhanden

**TODOs:**
- [ ] CSS-Selektoren an wg-gesucht.de anpassen

---

## ğŸ—„ï¸ Datenbank-Schema

### Tabelle: listings

| Spalte | Typ | Beschreibung |
|--------|-----|--------------|
| id | INTEGER | Auto-increment PK |
| listing_id | TEXT | Eindeutige ID (unique) |
| url | TEXT | URL zur Anzeige |
| title | TEXT | Titel |
| city | TEXT | Stadt |
| size | REAL | GrÃ¶ÃŸe in mÂ² |
| rent | REAL | Miete in â‚¬ |
| ... | ... | (weitere Felder siehe models.py) |

**Indizes:**
- `idx_city` auf `city`
- `idx_rent` auf `rent`
- `idx_scraped_at` auf `scraped_at`

---

## ğŸ”„ Workflow

### Entwicklung

```bash
# 1. Scraper anpassen
code src/wg_scraper/scraper.py

# 2. Selektoren in Config eintragen
code src/wg_scraper/config.py

# 3. Mit 1 Seite testen
.env/bin/wg-scraper -vv scrape --max-pages 1 "URL"

# 4. Tests schreiben
code tests/test_scraper.py
.env/bin/pytest

# 5. Produktiv nutzen
.env/bin/wg-scraper scrape "URL"
```

### Produktive Nutzung

```bash
# Scrapen
.env/bin/wg-scraper scrape "URL" --delay 2.0

# Auswerten
.env/bin/wg-scraper list --city Berlin
.env/bin/wg-scraper stats

# Backup
cp wg_data.db wg_data_backup_$(date +%Y%m%d).db
```

---

## ğŸ§© Dependencies

### Produktiv (install_requires)
- **click** â‰¥ 8.0: CLI-Framework
- **requests** â‰¥ 2.28.0: HTTP-Client
- **beautifulsoup4** â‰¥ 4.11.0: HTML-Parser
- **lxml** â‰¥ 4.9.0: XML/HTML-Parser

### Development (requirements-dev.txt)
- **pytest**: Testing
- **flake8**: Linting
- **black**: Code-Formatting
- **mypy**: Type-Checking

---

## ğŸ“Š Entry Points

Konfiguriert in `setup.cfg`:

```ini
[options.entry_points]
console_scripts =
    wg-scraper = wg_scraper.cli:run
```

Nach Installation verfÃ¼gbar als:
```bash
wg-scraper  # (im aktivierten venv)
.env/bin/wg-scraper  # (direkter Pfad)
```

---

## ğŸ“ Beispiel-Nutzung

### Via CLI
```bash
# Scrapen
.env/bin/wg-scraper scrape "https://www.wg-gesucht.de/..."

# Anzeigen
.env/bin/wg-scraper list

# Mit Filter
.env/bin/wg-scraper list --city MÃ¼nchen --limit 20
```

### Via Python API
```python
from wg_scraper.scraper import WGScraper
from wg_scraper.database import Database

# Scrapen
scraper = WGScraper(delay=1.5)
listings = scraper.scrape_search_results(url, max_pages=3)

# Speichern
db = Database()
db.init_db()
for listing in listings:
    db.save_listing(listing)

# Abfragen
berlin_listings = db.get_listings(city="Berlin", max_rent=500)
```

Siehe [examples.py](examples.py) fÃ¼r vollstÃ¤ndige Beispiele.

---

## ğŸ“ NÃ¤chste Schritte

### PrioritÃ¤t 1: Scraping implementieren
1. wg-gesucht.de im Browser analysieren
2. CSS-Selektoren identifizieren
3. `scraper.py` anpassen
4. Testen mit `--max-pages 1`

### PrioritÃ¤t 2: Tests
1. Test-Cases schreiben
2. Mock-Responses nutzen
3. Edge-Cases abdecken

### PrioritÃ¤t 3: Features
1. Export-Funktion (CSV, JSON)
2. Mehr Filter-Optionen
3. Benachrichtigungen bei neuen Anzeigen
4. Web-Interface (Flask/Streamlit)

---

## ğŸ”— Wichtige Dateien zum Starten

1. **[QUICKSTART.md](QUICKSTART.md)** - Schnellstart-Guide
2. **[README_USAGE.md](README_USAGE.md)** - VollstÃ¤ndige Dokumentation
3. **[examples.py](examples.py)** - Code-Beispiele
4. **[src/wg_scraper/scraper.py](src/wg_scraper/scraper.py)** - Hier wird implementiert!

---

**Projekt-Status:** ğŸŸ¢ Grundstruktur fertig | ğŸŸ¡ Scraping-Implementierung ausstehend

*Erstellt: 13. Februar 2026*
