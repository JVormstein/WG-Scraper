# WG-Gesucht Scraper

Genervt von wg-gesucht.de's suchinterface? Willst du eine tifere Analyse, oder einfach alle WGs mit offenen Stellen in einer Region? wg-scraper ist ein Python-CLI-Tool zum Scrapen von WG-Anzeigen von wg-gesucht.de und Lokaler Analyse der selbigen.

## Features

- ğŸ” **Web Scraping**: Automatisches Scrapen von WG-Anzeigen
- ğŸ“„ **Pagination**: Iteriert automatisch durch alle Suchergebnis-Seiten
- ğŸ’¾ **Datenbank**: SQLite-basierte lokale Datenspeicherung
- ğŸ“Š **Statistiken**: Analysiere gespeicherte Anzeigen
- ğŸ¨ **CLI**: Benutzerfreundliche Kommandozeilen-Schnittstelle
- â±ï¸ **Rate Limiting**: Konfigurierbare VerzÃ¶gerung zwischen Requests

## Installation

### Voraussetzungen

- Python 3.8 oder hÃ¶her
- pip

### Setup

1. **Virtual Environment erstellen (falls noch nicht vorhanden):**
   ```bash
   python -m venv .env
   ```

2. **Virtual Environment aktivieren:**
   ```bash
   # Linux/Mac
   source .env/bin/activate
   
   # Windows
   .env\Scripts\activate
   ```

3. **Paket installieren:**
   ```bash
   pip install -e .
   ```

## Nutzung

### Grundlegende Befehle

#### 1. WG-Anzeigen scrapen

Gehe einfach wg-gesucht.de, gebe deine Filter ein und kopiere die gesamte URL. Der Scraper erledigt den Rest.

```bash
wg-scraper scrape "https://www.wg-gesucht.de/wg-zimmer-in-Berlin.8.0.1.0.html"
```

Mit optionalen Parametern:

```bash
wg-scraper scrape \
  --db-path meine_datenbank.db \
  --max-pages 5 \
  --delay 2.0 \
  "https://www.wg-gesucht.de/wg-zimmer-in-Berlin.8.0.1.0.html"
```

**Parameter:**
- `--db-path PATH`: Pfad zur SQLite-Datenbank (Standard: `wg_data.db`)
- `--max-pages INTEGER`: Maximale Anzahl zu scrapender Seiten (Standard: alle)
- `--delay FLOAT`: VerzÃ¶gerung zwischen Requests in Sekunden (Standard: 1.0)

#### 2. Gespeicherte Anzeigen anzeigen

```bash
wg-scraper list
```

Mit Filtern:

```bash
wg-scraper list --limit 20 --city Berlin
```

**Parameter:**
- `--db-path PATH`: Pfad zur Datenbank
- `--limit INTEGER`: Anzahl anzuzeigender Anzeigen (Standard: 10)
- `--city TEXT`: Filter nach Stadt

#### 3. Statistiken anzeigen

```bash
wg-scraper stats
```

Zeigt an:
- Gesamtzahl der Anzeigen
- Anzahl unterschiedlicher StÃ¤dte
- Durchschnittliche Miete
- Durchschnittliche GrÃ¶ÃŸe
- Top 5 StÃ¤dte nach Anzahl

### Verbose-Modus

FÃ¼r detaillierte Ausgaben verwende die `-v` Option:

```bash
# Info-Level
wg-scraper -v scrape "URL"

# Debug-Level
wg-scraper -vv scrape "URL"
```

## Projektstruktur

```
WG Scraper/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ wg_scraper/
â”‚       â”œâ”€â”€ __init__.py      # Paket-Initialisierung
â”‚       â”œâ”€â”€ cli.py           # CLI-Befehle (Click)
â”‚       â”œâ”€â”€ scraper.py       # Web-Scraping-Logik
â”‚       â”œâ”€â”€ database.py      # Datenbank-Verwaltung
â”‚       â”œâ”€â”€ models.py        # Datenmodelle
â”‚       â””â”€â”€ skeleton.py      # PyScaffold-Beispiel (kann entfernt werden)
â”œâ”€â”€ tests/                   # Unit-Tests
â”œâ”€â”€ docs/                    # Dokumentation
â”œâ”€â”€ setup.cfg               # Paket-Konfiguration
â”œâ”€â”€ setup.py                # Setup-Script
â””â”€â”€ README.md               # Diese Datei
```

## Datenbank-Schema

Die SQLite-Datenbank verwendet folgendes Schema:

**Tabelle: listings**

| Feld | Typ | Beschreibung |
|------|-----|--------------|
| id | INTEGER | PrimÃ¤rschlÃ¼ssel (auto-increment) |
| listing_id | TEXT | Eindeutige ID von wg-gesucht.de |
| url | TEXT | URL zur Anzeige |
| title | TEXT | Titel der Anzeige |
| city | TEXT | Stadt |
| district | TEXT | Stadtteil/Bezirk |
| size | REAL | GrÃ¶ÃŸe in mÂ² |
| rent | REAL | Miete in Euro (warm) |
| available_from | TEXT | VerfÃ¼gbar ab |
| available_until | TEXT | VerfÃ¼gbar bis |
| room_type | TEXT | Zimmerart |
| online_since | TEXT | Online seit |
| description | TEXT | Beschreibungstext |
| flatmates | INTEGER | Anzahl Mitbewohner |
| flatmate_details | TEXT | Details zu Mitbewohnern |
| features | TEXT | Ausstattungsmerkmale (komma-separiert) |
| images | TEXT | Bild-URLs (komma-separiert) |
| contact_name | TEXT | Ansprechpartner |
| scraped_at | TEXT | Zeitpunkt des Scrapings |
| created_at | TIMESTAMP | DB-Eintrag erstellt am |

## Entwicklung

### Tests ausfÃ¼hren

```bash
# Alle Tests
pytest

# Mit Coverage
pytest --cov=wg_scraper

# Spezifische Tests
pytest tests/test_skeleton.py
```

### Code-Style prÃ¼fen

```bash
# Flake8
flake8 src/

# Black (Formatting)
black src/ tests/
```

## Dependencies

- **click**: CLI-Framework
- **requests**: HTTP-Requests
- **beautifulsoup4**: HTML-Parsing
- **lxml**: XML/HTML-Parser (schneller als html.parser)

## Lizenz

MIT License - siehe [LICENSE.txt](LICENSE.txt)
## Beitragen

Contributions sind willkommen! Bitte erstelle ein Issue oder Pull Request.
